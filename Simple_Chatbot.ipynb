{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Eq2CetO0py2Y",
        "outputId": "75a1d4e2-6f87-4ef2-db35-e937704a919c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ata science is an interdisciplinary academic field[1] that uses statistics, scientific computing, s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "text[1:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa74qdtip6H0"
      },
      "outputs": [],
      "source": [
        "\n",
        "text = text.lower()\n",
        "word_token = nltk.word_tokenize(text)\n",
        "sent_token = nltk.sent_tokenize(text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYdAMc3dr7Gh",
        "outputId": "80c211aa-429c-4902-9e78-4d1bcdc206af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'science']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "word_token[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mjtoTLzsnJS",
        "outputId": "7b34681d-b75a-4c73-be2f-170c231be728"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processes, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.',\n",
              " '[2]\\n\\ndata science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).',\n",
              " '[3] data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sent_token[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQeUjqPsyPa"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YatlscDsx2D"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import string\n",
        "\n",
        "lemmar= nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def toklemma(tokens):\n",
        "\n",
        "  return [lemmar.lemmatize(tk) for tk in tokens]\n",
        "\n",
        "remove_puc_dict= dict((ord(puc), None) for puc in string.punctuation)\n",
        "\n",
        "def lentiormalize(text):\n",
        "\n",
        "   return toklemma(nltk.word_tokenize(text.lower().translate(remove_puc_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUf4UTtBu2TK"
      },
      "source": [
        " Greet Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvQoc-03u9aZ"
      },
      "outputs": [],
      "source": [
        "greet_input = ('hey', 'belo', 'greetings', 'sup', 'whats up')\n",
        "\n",
        "greet_response = ['hey', 'helo', 'hi there', 'whats going on', 'I am glad! you are talking to me']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utKlCUsawVjg"
      },
      "outputs": [],
      "source": [
        "def greet(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in greet_input:\n",
        "            return random.choice(greet_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTWlm7UPw1Se"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  # hello my name is hello\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teAFxxpDw5iU"
      },
      "source": [
        "Checking similarity between user input and existed text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcs8JPK7w4fa"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def response(user_input):\n",
        "    roboi_res = \"\"\n",
        "    sent_token.append(user_input)\n",
        "\n",
        "    tfidfvec = TfidfVectorizer(tokenizer=lemNormalize, stop_words=\"english\")\n",
        "    tfidf = tfidfvec.fit_transform(sent_token)\n",
        "\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx = vals.argsort()[0][-2]\n",
        "\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "\n",
        "    req_tfidf = flat[-2]\n",
        "\n",
        "    if req_tfidf == 0:\n",
        "        roboi_res = roboi_res + \"I am sorry, I don't understand you.\"\n",
        "        return roboi_res\n",
        "    else:\n",
        "        roboi_res = roboi_res + sent_token[idx]\n",
        "        return roboi_res\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnNrKIP70eXp"
      },
      "source": [
        "Conversation Start End Protocols"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample data for chatbot responses\n",
        "with open('/content/chatbot.txt', 'r', errors='ignore') as file:\n",
        "    raw = file.read().lower()\n",
        "\n",
        "# Tokenization\n",
        "sent_tokens = nltk.sent_tokenize(raw)\n",
        "word_tokens = nltk.word_tokenize(raw)\n",
        "\n",
        "# Lemmatization\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "# Response generation\n",
        "def response(user_response):\n",
        "    robo_response = ''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', token_pattern=None)\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx = vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if req_tfidf == 0:\n",
        "        robo_response = \"I am sorry! I don't understand you.\"\n",
        "    else:\n",
        "        robo_response = sent_tokens[idx]\n",
        "    sent_tokens.remove(user_response)\n",
        "    return robo_response\n",
        "\n",
        "# Initialize the flag and greet the user\n",
        "flag = True\n",
        "print(\"Bot: Hey, I'm Noor Saeed. Let's have a conversation. If you want to exit just type 'Buy!'\")\n",
        "\n",
        "while flag:\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "\n",
        "    if user_response != 'buy':\n",
        "        if user_response in ['thanks', 'thank you']:\n",
        "            flag = False\n",
        "            print(\"Bot: You are welcome!\")\n",
        "        else:\n",
        "            print(\"Bot:\", end=\"\")\n",
        "            print(response(user_response))\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"Bot: Ok, Goodbye! Take care.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYQdK0UEqHKD",
        "outputId": "ad8199e7-c3c5-41a1-e620-5081774d7837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Hey, I'm Noor Saeed. Let's have a conversation. If you want to exit just type 'Buy!'\n",
            "Data science\n",
            "Bot:\"what is data science ?\n",
            "give me a brief\n",
            "Bot:I am sorry! I don't understand you.\n",
            "Foundations\n",
            "Bot:both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences.\n",
            "data scientist\n",
            "Bot:\"rise of the data scientist\".\n",
            "\n",
            "Bot:I am sorry! I don't understand you.\n",
            "Etymology\n",
            "Bot:[20]\n",
            "\n",
            "etymology\n",
            "early usage\n",
            "in 1962, john tukey described a field he called \"data analysis\", which resembles modern data science.\n",
            "Data science and data analysis\n",
            "Bot:\"what is data science ?\n",
            "Data science \n",
            "Bot:\"what is data science ?\n",
            "Cloud Computing for Data Science\n",
            "Bot:cloud computing for data science\n",
            "\n",
            "a cloud-based architecture for enabling big data analytics.\n",
            "Ethical consideration in Data Science\n",
            "Bot:[42]\n",
            "\n",
            "ethical consideration in data science\n",
            "data science involve collecting, processing, and analyzing data which often including personal and sensitive information.\n",
            "buy\n",
            "Bot: Ok, Goodbye! Take care.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}